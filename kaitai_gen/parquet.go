// Code generated by kaitai-struct-compiler from a .ksy source file. DO NOT EDIT.

package kaitai_gen

import (
	"github.com/kaitai-io/kaitai_struct_go_runtime/kaitai"
	"io"
	"bytes"
)


/**
 * Apache Parquet is a columnar storage format designed for efficient
 * data storage and retrieval. This specification describes the binary
 * file format structure.
 * 
 * File structure:
 * - Magic "PAR1" (4 bytes) at start
 * - Data blocks (row groups with column chunks)
 * - Footer length (4 bytes, little-endian)
 * - Footer (Thrift-encoded FileMetaData)
 * - Magic "PAR1" (4 bytes) at end
 * 
 * Note: The footer is Thrift-encoded binary data, which uses variable-length
 * encoding and field tags. The type definitions below represent the logical
 * structure of the metadata, but parsing the actual Thrift-encoded footer
 * requires a Thrift parser or manual decoding.
 */

type Parquet_ColumnOrderType int
const (
	Parquet_ColumnOrderType__TypeOrder Parquet_ColumnOrderType = 0
	Parquet_ColumnOrderType__ColumnOrderTypeUndefined Parquet_ColumnOrderType = 1
)
var values_Parquet_ColumnOrderType = map[Parquet_ColumnOrderType]struct{}{0: {}, 1: {}}
func (v Parquet_ColumnOrderType) isDefined() bool {
	_, ok := values_Parquet_ColumnOrderType[v]
	return ok
}

type Parquet_CompressionCodec int
const (
	Parquet_CompressionCodec__Uncompressed Parquet_CompressionCodec = 0
	Parquet_CompressionCodec__Snappy Parquet_CompressionCodec = 1
	Parquet_CompressionCodec__Gzip Parquet_CompressionCodec = 2
	Parquet_CompressionCodec__Lzo Parquet_CompressionCodec = 3
	Parquet_CompressionCodec__Brotli Parquet_CompressionCodec = 4
	Parquet_CompressionCodec__Lz4 Parquet_CompressionCodec = 5
	Parquet_CompressionCodec__Zstd Parquet_CompressionCodec = 6
	Parquet_CompressionCodec__Lz4Raw Parquet_CompressionCodec = 7
)
var values_Parquet_CompressionCodec = map[Parquet_CompressionCodec]struct{}{0: {}, 1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}}
func (v Parquet_CompressionCodec) isDefined() bool {
	_, ok := values_Parquet_CompressionCodec[v]
	return ok
}

type Parquet_ConvertedTypeEnum int
const (
	Parquet_ConvertedTypeEnum__Utf8 Parquet_ConvertedTypeEnum = 0
	Parquet_ConvertedTypeEnum__Map Parquet_ConvertedTypeEnum = 1
	Parquet_ConvertedTypeEnum__MapKeyValue Parquet_ConvertedTypeEnum = 2
	Parquet_ConvertedTypeEnum__List Parquet_ConvertedTypeEnum = 3
	Parquet_ConvertedTypeEnum__Enum Parquet_ConvertedTypeEnum = 4
	Parquet_ConvertedTypeEnum__Decimal Parquet_ConvertedTypeEnum = 5
	Parquet_ConvertedTypeEnum__Date Parquet_ConvertedTypeEnum = 6
	Parquet_ConvertedTypeEnum__TimeMillis Parquet_ConvertedTypeEnum = 7
	Parquet_ConvertedTypeEnum__TimeMicros Parquet_ConvertedTypeEnum = 8
	Parquet_ConvertedTypeEnum__TimestampMillis Parquet_ConvertedTypeEnum = 9
	Parquet_ConvertedTypeEnum__TimestampMicros Parquet_ConvertedTypeEnum = 10
	Parquet_ConvertedTypeEnum__Uint8 Parquet_ConvertedTypeEnum = 11
	Parquet_ConvertedTypeEnum__Uint16 Parquet_ConvertedTypeEnum = 12
	Parquet_ConvertedTypeEnum__Uint32 Parquet_ConvertedTypeEnum = 13
	Parquet_ConvertedTypeEnum__Uint64 Parquet_ConvertedTypeEnum = 14
	Parquet_ConvertedTypeEnum__Int8 Parquet_ConvertedTypeEnum = 15
	Parquet_ConvertedTypeEnum__Int16 Parquet_ConvertedTypeEnum = 16
	Parquet_ConvertedTypeEnum__Int32 Parquet_ConvertedTypeEnum = 17
	Parquet_ConvertedTypeEnum__Int64 Parquet_ConvertedTypeEnum = 18
	Parquet_ConvertedTypeEnum__Json Parquet_ConvertedTypeEnum = 19
	Parquet_ConvertedTypeEnum__Bson Parquet_ConvertedTypeEnum = 20
	Parquet_ConvertedTypeEnum__Interval Parquet_ConvertedTypeEnum = 21
)
var values_Parquet_ConvertedTypeEnum = map[Parquet_ConvertedTypeEnum]struct{}{0: {}, 1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {}, 16: {}, 17: {}, 18: {}, 19: {}, 20: {}, 21: {}}
func (v Parquet_ConvertedTypeEnum) isDefined() bool {
	_, ok := values_Parquet_ConvertedTypeEnum[v]
	return ok
}

type Parquet_EncodingEnum int
const (
	Parquet_EncodingEnum__Plain Parquet_EncodingEnum = 0
	Parquet_EncodingEnum__PlainDictionary Parquet_EncodingEnum = 1
	Parquet_EncodingEnum__Rle Parquet_EncodingEnum = 2
	Parquet_EncodingEnum__BitPacked Parquet_EncodingEnum = 3
	Parquet_EncodingEnum__DeltaBinaryPacked Parquet_EncodingEnum = 4
	Parquet_EncodingEnum__DeltaLengthByteArray Parquet_EncodingEnum = 5
	Parquet_EncodingEnum__DeltaByteArray Parquet_EncodingEnum = 6
	Parquet_EncodingEnum__RleDictionary Parquet_EncodingEnum = 7
	Parquet_EncodingEnum__ByteStreamSplit Parquet_EncodingEnum = 8
)
var values_Parquet_EncodingEnum = map[Parquet_EncodingEnum]struct{}{0: {}, 1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}}
func (v Parquet_EncodingEnum) isDefined() bool {
	_, ok := values_Parquet_EncodingEnum[v]
	return ok
}

type Parquet_FieldRepetitionType int
const (
	Parquet_FieldRepetitionType__Required Parquet_FieldRepetitionType = 0
	Parquet_FieldRepetitionType__Optional Parquet_FieldRepetitionType = 1
	Parquet_FieldRepetitionType__Repeated Parquet_FieldRepetitionType = 2
)
var values_Parquet_FieldRepetitionType = map[Parquet_FieldRepetitionType]struct{}{0: {}, 1: {}, 2: {}}
func (v Parquet_FieldRepetitionType) isDefined() bool {
	_, ok := values_Parquet_FieldRepetitionType[v]
	return ok
}

type Parquet_PageTypeEnum int
const (
	Parquet_PageTypeEnum__DataPage Parquet_PageTypeEnum = 0
	Parquet_PageTypeEnum__IndexPage Parquet_PageTypeEnum = 1
	Parquet_PageTypeEnum__DictionaryPage Parquet_PageTypeEnum = 2
	Parquet_PageTypeEnum__DataPageV2 Parquet_PageTypeEnum = 3
)
var values_Parquet_PageTypeEnum = map[Parquet_PageTypeEnum]struct{}{0: {}, 1: {}, 2: {}, 3: {}}
func (v Parquet_PageTypeEnum) isDefined() bool {
	_, ok := values_Parquet_PageTypeEnum[v]
	return ok
}

type Parquet_TypeEnum int
const (
	Parquet_TypeEnum__Boolean Parquet_TypeEnum = 0
	Parquet_TypeEnum__Int32 Parquet_TypeEnum = 1
	Parquet_TypeEnum__Int64 Parquet_TypeEnum = 2
	Parquet_TypeEnum__Int96 Parquet_TypeEnum = 3
	Parquet_TypeEnum__Float Parquet_TypeEnum = 4
	Parquet_TypeEnum__Double Parquet_TypeEnum = 5
	Parquet_TypeEnum__ByteArray Parquet_TypeEnum = 6
	Parquet_TypeEnum__FixedLenByteArray Parquet_TypeEnum = 7
)
var values_Parquet_TypeEnum = map[Parquet_TypeEnum]struct{}{0: {}, 1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}}
func (v Parquet_TypeEnum) isDefined() bool {
	_, ok := values_Parquet_TypeEnum[v]
	return ok
}
type Parquet struct {
	Magic string
	_io *kaitai.Stream
	_root *Parquet
	_parent kaitai.Struct
	_raw_data []byte
	_raw_footerThrift []byte
	_f_data bool
	data *Parquet_DataSection
	_f_footerLength bool
	footerLength uint32
	_f_footerRaw bool
	footerRaw []uint8
	_f_footerThrift bool
	footerThrift *ThriftCompact_CompactStruct
	_f_magicEnd bool
	magicEnd string
}
func NewParquet() *Parquet {
	return &Parquet{
	}
}

func (this Parquet) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet) Read(io *kaitai.Stream, parent kaitai.Struct, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp1, err := this._io.ReadBytes(int(4))
	if err != nil {
		return err
	}
	tmp1 = tmp1
	this.Magic = string(tmp1)
	return err
}

/**
 * Data section containing row groups
 */
func (this *Parquet) Data() (v *Parquet_DataSection, err error) {
	if (this._f_data) {
		return this.data, nil
	}
	this._f_data = true
	_pos, err := this._io.Pos()
	if err != nil {
		return nil, err
	}
	_, err = this._io.Seek(int64(4), io.SeekStart)
	if err != nil {
		return nil, err
	}
	tmp2, err := this._io.Size()
	if err != nil {
		return nil, err
	}
	tmp3, err := this.FooterLength()
	if err != nil {
		return nil, err
	}
	tmp4, err := this._io.ReadBytes(int(((tmp2 - 8) - int64(tmp3)) - 4))
	if err != nil {
		return nil, err
	}
	tmp4 = tmp4
	this._raw_data = tmp4
	_io__raw_data := kaitai.NewStream(bytes.NewReader(this._raw_data))
	tmp5 := NewParquet_DataSection()
	err = tmp5.Read(_io__raw_data, this, this._root)
	if err != nil {
		return nil, err
	}
	this.data = tmp5
	_, err = this._io.Seek(_pos, io.SeekStart)
	if err != nil {
		return nil, err
	}
	return this.data, nil
}

/**
 * Length of the footer in bytes (32-bit little-endian), stored before the final magic.
 */
func (this *Parquet) FooterLength() (v uint32, err error) {
	if (this._f_footerLength) {
		return this.footerLength, nil
	}
	this._f_footerLength = true
	_pos, err := this._io.Pos()
	if err != nil {
		return 0, err
	}
	tmp6, err := this._io.Size()
	if err != nil {
		return 0, err
	}
	_, err = this._io.Seek(int64(tmp6 - 8), io.SeekStart)
	if err != nil {
		return 0, err
	}
	tmp7, err := this._io.ReadU4le()
	if err != nil {
		return 0, err
	}
	this.footerLength = tmp7
	_, err = this._io.Seek(_pos, io.SeekStart)
	if err != nil {
		return 0, err
	}
	return this.footerLength, nil
}

/**
 * Raw footer data (Thrift-encoded FileMetaData)
 */
func (this *Parquet) FooterRaw() (v []uint8, err error) {
	if (this._f_footerRaw) {
		return this.footerRaw, nil
	}
	this._f_footerRaw = true
	_pos, err := this._io.Pos()
	if err != nil {
		return nil, err
	}
	tmp8, err := this._io.Size()
	if err != nil {
		return nil, err
	}
	tmp9, err := this.FooterLength()
	if err != nil {
		return nil, err
	}
	_, err = this._io.Seek(int64((tmp8 - 8) - int64(tmp9)), io.SeekStart)
	if err != nil {
		return nil, err
	}
	tmp10, err := this.FooterLength()
	if err != nil {
		return nil, err
	}
	for i := 0; i < int(tmp10); i++ {
		_ = i
		tmp11, err := this._io.ReadU1()
		if err != nil {
			return nil, err
		}
		this.footerRaw = append(this.footerRaw, tmp11)
	}
	_, err = this._io.Seek(_pos, io.SeekStart)
	if err != nil {
		return nil, err
	}
	return this.footerRaw, nil
}

/**
 * Footer parsed as a Thrift Compact Protocol struct.
 */
func (this *Parquet) FooterThrift() (v *ThriftCompact_CompactStruct, err error) {
	if (this._f_footerThrift) {
		return this.footerThrift, nil
	}
	this._f_footerThrift = true
	_pos, err := this._io.Pos()
	if err != nil {
		return nil, err
	}
	tmp12, err := this._io.Size()
	if err != nil {
		return nil, err
	}
	tmp13, err := this.FooterLength()
	if err != nil {
		return nil, err
	}
	_, err = this._io.Seek(int64((tmp12 - 8) - int64(tmp13)), io.SeekStart)
	if err != nil {
		return nil, err
	}
	tmp14, err := this.FooterLength()
	if err != nil {
		return nil, err
	}
	tmp15, err := this._io.ReadBytes(int(tmp14))
	if err != nil {
		return nil, err
	}
	tmp15 = tmp15
	this._raw_footerThrift = tmp15
	_io__raw_footerThrift := kaitai.NewStream(bytes.NewReader(this._raw_footerThrift))
	tmp16 := NewThriftCompact_CompactStruct()
	err = tmp16.Read(_io__raw_footerThrift, nil, nil)
	if err != nil {
		return nil, err
	}
	this.footerThrift = tmp16
	_, err = this._io.Seek(_pos, io.SeekStart)
	if err != nil {
		return nil, err
	}
	return this.footerThrift, nil
}

/**
 * Magic number "PAR1" at the end of the file
 */
func (this *Parquet) MagicEnd() (v string, err error) {
	if (this._f_magicEnd) {
		return this.magicEnd, nil
	}
	this._f_magicEnd = true
	_pos, err := this._io.Pos()
	if err != nil {
		return "", err
	}
	tmp17, err := this._io.Size()
	if err != nil {
		return "", err
	}
	_, err = this._io.Seek(int64(tmp17 - 4), io.SeekStart)
	if err != nil {
		return "", err
	}
	tmp18, err := this._io.ReadBytes(int(4))
	if err != nil {
		return "", err
	}
	tmp18 = tmp18
	this.magicEnd = string(tmp18)
	_, err = this._io.Seek(_pos, io.SeekStart)
	if err != nil {
		return "", err
	}
	return this.magicEnd, nil
}

/**
 * Magic number "PAR1" at the beginning of the file
 */

/**
 * A column chunk containing the actual data pages. The data
 * may be compressed and encoded according to the metadata.
 */
type Parquet_ColumnChunk struct {
	Pages []*Parquet_Page
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_RowGroup
}
func NewParquet_ColumnChunk() *Parquet_ColumnChunk {
	return &Parquet_ColumnChunk{
	}
}

func (this Parquet_ColumnChunk) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_ColumnChunk) Read(io *kaitai.Stream, parent *Parquet_RowGroup, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	for i := 0;; i++ {
		tmp19, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp19 {
			break
		}
		tmp20 := NewParquet_Page()
		err = tmp20.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.Pages = append(this.Pages, tmp20)
	}
	return err
}

/**
 * Pages in this column chunk
 */

/**
 * Metadata for a column chunk. This is a simplified representation
 * of the Thrift-encoded ColumnChunk structure.
 */
type Parquet_ColumnChunkMetadata struct {
	Type Parquet_TypeEnum
	Encodings []Parquet_EncodingEnum
	PathInSchema []string
	Codec Parquet_CompressionCodec
	NumValues uint64
	TotalUncompressedSize uint64
	TotalCompressedSize uint64
	KeyValueMetadata []*Parquet_KeyValue
	DataPageOffset uint64
	IndexPageOffset uint64
	DictionaryPageOffset uint64
	Statistics *Parquet_Statistics
	EncodingStats []*Parquet_PageEncodingStats
	BloomFilterOffset uint64
	BloomFilterLength uint32
	SizeStatistics *Parquet_SizeStatistics
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_RowGroupMetadata
}
func NewParquet_ColumnChunkMetadata() *Parquet_ColumnChunkMetadata {
	return &Parquet_ColumnChunkMetadata{
	}
}

func (this Parquet_ColumnChunkMetadata) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_ColumnChunkMetadata) Read(io *kaitai.Stream, parent *Parquet_RowGroupMetadata, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp21, err := this._io.ReadU1()
	if err != nil {
		return err
	}
	this.Type = Parquet_TypeEnum(tmp21)
	for i := 0;; i++ {
		tmp22, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp22 {
			break
		}
		tmp23, err := this._io.ReadU1()
		if err != nil {
			return err
		}
		this.Encodings = append(this.Encodings, Parquet_EncodingEnum(tmp23))
	}
	for i := 0;; i++ {
		tmp24, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp24 {
			break
		}
		tmp25, err := this._io.ReadBytesTerm(0, false, true, true)
		if err != nil {
			return err
		}
		this.PathInSchema = append(this.PathInSchema, string(tmp25))
	}
	tmp26, err := this._io.ReadU1()
	if err != nil {
		return err
	}
	this.Codec = Parquet_CompressionCodec(tmp26)
	tmp27, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.NumValues = uint64(tmp27)
	tmp28, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.TotalUncompressedSize = uint64(tmp28)
	tmp29, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.TotalCompressedSize = uint64(tmp29)
	for i := 0;; i++ {
		tmp30, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp30 {
			break
		}
		tmp31 := NewParquet_KeyValue()
		err = tmp31.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.KeyValueMetadata = append(this.KeyValueMetadata, tmp31)
	}
	tmp32, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.DataPageOffset = uint64(tmp32)
	tmp33, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.IndexPageOffset = uint64(tmp33)
	tmp34, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.DictionaryPageOffset = uint64(tmp34)
	tmp35 := NewParquet_Statistics()
	err = tmp35.Read(this._io, this, this._root)
	if err != nil {
		return err
	}
	this.Statistics = tmp35
	for i := 0;; i++ {
		tmp36, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp36 {
			break
		}
		tmp37 := NewParquet_PageEncodingStats()
		err = tmp37.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.EncodingStats = append(this.EncodingStats, tmp37)
	}
	tmp38, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.BloomFilterOffset = uint64(tmp38)
	tmp39, err := this._io.ReadU4le()
	if err != nil {
		return err
	}
	this.BloomFilterLength = uint32(tmp39)
	tmp40 := NewParquet_SizeStatistics()
	err = tmp40.Read(this._io, this, this._root)
	if err != nil {
		return err
	}
	this.SizeStatistics = tmp40
	return err
}

/**
 * Data type of the column
 */

/**
 * Encodings used in this column
 */

/**
 * Path in the schema
 */

/**
 * Compression codec used
 */

/**
 * Number of values in this column chunk
 */

/**
 * Total uncompressed size
 */

/**
 * Total compressed size
 */

/**
 * Key-value metadata pairs
 */

/**
 * Offset of the first data page
 */

/**
 * Offset of the index page (if present)
 */

/**
 * Offset of the dictionary page (if present)
 */

/**
 * Statistics for this column chunk
 */

/**
 * Encoding statistics
 */

/**
 * Bloom filter offset (if present)
 */

/**
 * Bloom filter length (if present)
 */

/**
 * Size statistics
 */

/**
 * Column order information
 */
type Parquet_ColumnOrder struct {
	Type Parquet_ColumnOrderType
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_FileMetadata
}
func NewParquet_ColumnOrder() *Parquet_ColumnOrder {
	return &Parquet_ColumnOrder{
	}
}

func (this Parquet_ColumnOrder) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_ColumnOrder) Read(io *kaitai.Stream, parent *Parquet_FileMetadata, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp41, err := this._io.ReadU1()
	if err != nil {
		return err
	}
	this.Type = Parquet_ColumnOrderType(tmp41)
	return err
}

/**
 * Type of column order
 */

/**
 * Data section containing row groups. The actual row groups are located
 * using offsets stored in the footer metadata. This section contains
 * the raw binary data that needs to be parsed according to the footer.
 */
type Parquet_DataSection struct {
	RawData []uint8
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet
}
func NewParquet_DataSection() *Parquet_DataSection {
	return &Parquet_DataSection{
	}
}

func (this Parquet_DataSection) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_DataSection) Read(io *kaitai.Stream, parent *Parquet, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	for i := 0;; i++ {
		tmp42, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp42 {
			break
		}
		tmp43, err := this._io.ReadU1()
		if err != nil {
			return err
		}
		this.RawData = append(this.RawData, tmp43)
	}
	return err
}

/**
 * Raw binary data containing row groups and column chunks
 */
type Parquet_FileMetadata struct {
	Version uint32
	Schema []*Parquet_SchemaElement
	NumRows uint64
	RowGroupsMeta []*Parquet_RowGroupMetadata
	KeyValueMetadata []*Parquet_KeyValue
	CreatedBy string
	ColumnOrders []*Parquet_ColumnOrder
	_io *kaitai.Stream
	_root *Parquet
	_parent kaitai.Struct
}
func NewParquet_FileMetadata() *Parquet_FileMetadata {
	return &Parquet_FileMetadata{
	}
}

func (this Parquet_FileMetadata) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_FileMetadata) Read(io *kaitai.Stream, parent kaitai.Struct, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp44, err := this._io.ReadU4le()
	if err != nil {
		return err
	}
	this.Version = uint32(tmp44)
	for i := 0;; i++ {
		tmp45, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp45 {
			break
		}
		tmp46 := NewParquet_SchemaElement()
		err = tmp46.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.Schema = append(this.Schema, tmp46)
	}
	tmp47, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.NumRows = uint64(tmp47)
	for i := 0;; i++ {
		tmp48, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp48 {
			break
		}
		tmp49 := NewParquet_RowGroupMetadata()
		err = tmp49.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.RowGroupsMeta = append(this.RowGroupsMeta, tmp49)
	}
	for i := 0;; i++ {
		tmp50, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp50 {
			break
		}
		tmp51 := NewParquet_KeyValue()
		err = tmp51.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.KeyValueMetadata = append(this.KeyValueMetadata, tmp51)
	}
	tmp52, err := this._io.ReadBytesTerm(0, false, true, true)
	if err != nil {
		return err
	}
	this.CreatedBy = string(tmp52)
	for i := 0;; i++ {
		tmp53, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp53 {
			break
		}
		tmp54 := NewParquet_ColumnOrder()
		err = tmp54.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.ColumnOrders = append(this.ColumnOrders, tmp54)
	}
	return err
}

/**
 * Version of the file format
 */

/**
 * Schema elements (Thrift-encoded)
 */

/**
 * Number of rows in the file
 */

/**
 * Row group metadata (Thrift-encoded)
 */

/**
 * Key-value metadata pairs (Thrift-encoded)
 */

/**
 * String identifying the library that created the file
 */

/**
 * Column orders (Thrift-encoded)
 */

/**
 * Key-value metadata pair
 */
type Parquet_KeyValue struct {
	Key string
	Value string
	_io *kaitai.Stream
	_root *Parquet
	_parent kaitai.Struct
}
func NewParquet_KeyValue() *Parquet_KeyValue {
	return &Parquet_KeyValue{
	}
}

func (this Parquet_KeyValue) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_KeyValue) Read(io *kaitai.Stream, parent kaitai.Struct, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp55, err := this._io.ReadBytesTerm(0, false, true, true)
	if err != nil {
		return err
	}
	this.Key = string(tmp55)
	tmp56, err := this._io.ReadBytesTerm(0, false, true, true)
	if err != nil {
		return err
	}
	this.Value = string(tmp56)
	return err
}

/**
 * Key name
 */

/**
 * Value string
 */

/**
 * A page is the unit of storage in a column chunk. Pages can be
 * data pages, dictionary pages, or index pages.
 */
type Parquet_Page struct {
	PageType Parquet_PageTypeEnum
	UncompressedPageSize uint32
	CompressedPageSize uint32
	Crc uint32
	Data []byte
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_ColumnChunk
}
func NewParquet_Page() *Parquet_Page {
	return &Parquet_Page{
	}
}

func (this Parquet_Page) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_Page) Read(io *kaitai.Stream, parent *Parquet_ColumnChunk, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp57, err := this._io.ReadU1()
	if err != nil {
		return err
	}
	this.PageType = Parquet_PageTypeEnum(tmp57)
	tmp58, err := this._io.ReadU4le()
	if err != nil {
		return err
	}
	this.UncompressedPageSize = uint32(tmp58)
	tmp59, err := this._io.ReadU4le()
	if err != nil {
		return err
	}
	this.CompressedPageSize = uint32(tmp59)
	tmp60, err := this._io.ReadU4le()
	if err != nil {
		return err
	}
	this.Crc = uint32(tmp60)
	tmp61, err := this._io.ReadBytes(int(this.CompressedPageSize))
	if err != nil {
		return err
	}
	tmp61 = tmp61
	this.Data = tmp61
	return err
}

/**
 * Type of the page
 */

/**
 * Uncompressed size of the page
 */

/**
 * Compressed size of the page
 */

/**
 * CRC32 checksum (present if file format version >= 2, requires footer parsing to determine)
 */

/**
 * Compressed page data
 */

/**
 * Encoding statistics for a page
 */
type Parquet_PageEncodingStats struct {
	PageType Parquet_PageTypeEnum
	Encoding Parquet_EncodingEnum
	Count uint32
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_ColumnChunkMetadata
}
func NewParquet_PageEncodingStats() *Parquet_PageEncodingStats {
	return &Parquet_PageEncodingStats{
	}
}

func (this Parquet_PageEncodingStats) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_PageEncodingStats) Read(io *kaitai.Stream, parent *Parquet_ColumnChunkMetadata, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp62, err := this._io.ReadU1()
	if err != nil {
		return err
	}
	this.PageType = Parquet_PageTypeEnum(tmp62)
	tmp63, err := this._io.ReadU1()
	if err != nil {
		return err
	}
	this.Encoding = Parquet_EncodingEnum(tmp63)
	tmp64, err := this._io.ReadU4le()
	if err != nil {
		return err
	}
	this.Count = uint32(tmp64)
	return err
}

/**
 * Type of page
 */

/**
 * Encoding used
 */

/**
 * Count of pages with this encoding
 */

/**
 * A row group containing column chunks. The actual data is stored
 * in column chunks which may be compressed and encoded.
 */
type Parquet_RowGroup struct {
	ColumnChunks []*Parquet_ColumnChunk
	_io *kaitai.Stream
	_root *Parquet
	_parent kaitai.Struct
}
func NewParquet_RowGroup() *Parquet_RowGroup {
	return &Parquet_RowGroup{
	}
}

func (this Parquet_RowGroup) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_RowGroup) Read(io *kaitai.Stream, parent kaitai.Struct, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	for i := 0;; i++ {
		tmp65, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp65 {
			break
		}
		tmp66 := NewParquet_ColumnChunk()
		err = tmp66.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.ColumnChunks = append(this.ColumnChunks, tmp66)
	}
	return err
}

/**
 * Column chunks in this row group
 */

/**
 * Metadata for a row group. This is a simplified representation
 * of the Thrift-encoded RowGroup structure.
 */
type Parquet_RowGroupMetadata struct {
	Columns []*Parquet_ColumnChunkMetadata
	TotalByteSize uint64
	NumRows uint64
	SortingColumns []*Parquet_SortingColumn
	FileOffset uint64
	TotalCompressedSize uint64
	Ordinal int32
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_FileMetadata
}
func NewParquet_RowGroupMetadata() *Parquet_RowGroupMetadata {
	return &Parquet_RowGroupMetadata{
	}
}

func (this Parquet_RowGroupMetadata) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_RowGroupMetadata) Read(io *kaitai.Stream, parent *Parquet_FileMetadata, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	for i := 0;; i++ {
		tmp67, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp67 {
			break
		}
		tmp68 := NewParquet_ColumnChunkMetadata()
		err = tmp68.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.Columns = append(this.Columns, tmp68)
	}
	tmp69, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.TotalByteSize = uint64(tmp69)
	tmp70, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.NumRows = uint64(tmp70)
	for i := 0;; i++ {
		tmp71, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp71 {
			break
		}
		tmp72 := NewParquet_SortingColumn()
		err = tmp72.Read(this._io, this, this._root)
		if err != nil {
			return err
		}
		this.SortingColumns = append(this.SortingColumns, tmp72)
	}
	tmp73, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.FileOffset = uint64(tmp73)
	tmp74, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.TotalCompressedSize = uint64(tmp74)
	tmp75, err := this._io.ReadS4le()
	if err != nil {
		return err
	}
	this.Ordinal = int32(tmp75)
	return err
}

/**
 * Column chunk metadata
 */

/**
 * Total byte size of all column chunks
 */

/**
 * Number of rows in this row group
 */

/**
 * Sorting columns information
 */

/**
 * File offset of the row group
 */

/**
 * Total compressed size
 */

/**
 * Ordinal of the row group
 */

/**
 * Schema element describing a column or nested structure.
 * This is a simplified representation; actual Thrift encoding
 * is more complex with variable-length fields.
 */
type Parquet_SchemaElement struct {
	Type Parquet_TypeEnum
	TypeLength int32
	RepetitionType Parquet_FieldRepetitionType
	Name string
	NumChildren int32
	ConvertedType Parquet_ConvertedTypeEnum
	Scale int32
	Precision int32
	FieldId int32
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_FileMetadata
}
func NewParquet_SchemaElement() *Parquet_SchemaElement {
	return &Parquet_SchemaElement{
	}
}

func (this Parquet_SchemaElement) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_SchemaElement) Read(io *kaitai.Stream, parent *Parquet_FileMetadata, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp76, err := this._io.ReadU1()
	if err != nil {
		return err
	}
	this.Type = Parquet_TypeEnum(tmp76)
	if (this.Type == Parquet_TypeEnum__FixedLenByteArray) {
		tmp77, err := this._io.ReadS4le()
		if err != nil {
			return err
		}
		this.TypeLength = int32(tmp77)
	}
	tmp78, err := this._io.ReadU1()
	if err != nil {
		return err
	}
	this.RepetitionType = Parquet_FieldRepetitionType(tmp78)
	tmp79, err := this._io.ReadBytesTerm(0, false, true, true)
	if err != nil {
		return err
	}
	this.Name = string(tmp79)
	tmp80, err := this._io.ReadS4le()
	if err != nil {
		return err
	}
	this.NumChildren = int32(tmp80)
	tmp81, err := this._io.ReadU1()
	if err != nil {
		return err
	}
	this.ConvertedType = Parquet_ConvertedTypeEnum(tmp81)
	tmp82, err := this._io.ReadS4le()
	if err != nil {
		return err
	}
	this.Scale = int32(tmp82)
	tmp83, err := this._io.ReadS4le()
	if err != nil {
		return err
	}
	this.Precision = int32(tmp83)
	tmp84, err := this._io.ReadS4le()
	if err != nil {
		return err
	}
	this.FieldId = int32(tmp84)
	return err
}

/**
 * Data type of the element
 */

/**
 * Length for FIXED_LEN_BYTE_ARRAY type
 */

/**
 * Repetition type (required, optional, repeated)
 */

/**
 * Name of the schema element
 */

/**
 * Number of child elements
 */

/**
 * Converted type for logical types
 */

/**
 * Scale for DECIMAL type
 */

/**
 * Precision for DECIMAL type
 */

/**
 * Field ID
 */

/**
 * Size statistics
 */
type Parquet_SizeStatistics struct {
	UnencodedByteArrayDataBytes uint64
	RepetitionLevelHistogram []uint64
	DefinitionLevelHistogram []uint64
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_ColumnChunkMetadata
}
func NewParquet_SizeStatistics() *Parquet_SizeStatistics {
	return &Parquet_SizeStatistics{
	}
}

func (this Parquet_SizeStatistics) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_SizeStatistics) Read(io *kaitai.Stream, parent *Parquet_ColumnChunkMetadata, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp85, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.UnencodedByteArrayDataBytes = uint64(tmp85)
	for i := 0;; i++ {
		tmp86, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp86 {
			break
		}
		tmp87, err := this._io.ReadU8le()
		if err != nil {
			return err
		}
		this.RepetitionLevelHistogram = append(this.RepetitionLevelHistogram, tmp87)
	}
	for i := 0;; i++ {
		tmp88, err := this._io.EOF()
		if err != nil {
			return err
		}
		if tmp88 {
			break
		}
		tmp89, err := this._io.ReadU8le()
		if err != nil {
			return err
		}
		this.DefinitionLevelHistogram = append(this.DefinitionLevelHistogram, tmp89)
	}
	return err
}

/**
 * Unencoded byte array data bytes
 */

/**
 * Repetition level histogram
 */

/**
 * Definition level histogram
 */

/**
 * Information about sorting columns
 */
type Parquet_SortingColumn struct {
	ColumnIdx uint32
	Descending bool
	NullsFirst bool
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_RowGroupMetadata
}
func NewParquet_SortingColumn() *Parquet_SortingColumn {
	return &Parquet_SortingColumn{
	}
}

func (this Parquet_SortingColumn) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_SortingColumn) Read(io *kaitai.Stream, parent *Parquet_RowGroupMetadata, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	tmp90, err := this._io.ReadU4le()
	if err != nil {
		return err
	}
	this.ColumnIdx = uint32(tmp90)
	tmp91, err := this._io.ReadBitsIntBe(1)
	if err != nil {
		return err
	}
	this.Descending = tmp91 != 0
	tmp92, err := this._io.ReadBitsIntBe(1)
	if err != nil {
		return err
	}
	this.NullsFirst = tmp92 != 0
	return err
}

/**
 * Column index
 */

/**
 * Whether sorting is descending
 */

/**
 * Whether nulls come first
 */

/**
 * Statistics for a column chunk, including min/max values,
 * null count, distinct count, etc.
 */
type Parquet_Statistics struct {
	Max []uint8
	Min []uint8
	NullCount uint64
	DistinctCount uint64
	MaxValue []uint8
	MinValue []uint8
	_io *kaitai.Stream
	_root *Parquet
	_parent *Parquet_ColumnChunkMetadata
}
func NewParquet_Statistics() *Parquet_Statistics {
	return &Parquet_Statistics{
	}
}

func (this Parquet_Statistics) IO_() *kaitai.Stream {
	return this._io
}

func (this *Parquet_Statistics) Read(io *kaitai.Stream, parent *Parquet_ColumnChunkMetadata, root *Parquet) (err error) {
	this._io = io
	this._parent = parent
	this._root = root

	for i := 0; i < int(16); i++ {
		_ = i
		tmp93, err := this._io.ReadU1()
		if err != nil {
			return err
		}
		this.Max = append(this.Max, tmp93)
	}
	for i := 0; i < int(16); i++ {
		_ = i
		tmp94, err := this._io.ReadU1()
		if err != nil {
			return err
		}
		this.Min = append(this.Min, tmp94)
	}
	tmp95, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.NullCount = uint64(tmp95)
	tmp96, err := this._io.ReadU8le()
	if err != nil {
		return err
	}
	this.DistinctCount = uint64(tmp96)
	for i := 0; i < int(16); i++ {
		_ = i
		tmp97, err := this._io.ReadU1()
		if err != nil {
			return err
		}
		this.MaxValue = append(this.MaxValue, tmp97)
	}
	for i := 0; i < int(16); i++ {
		_ = i
		tmp98, err := this._io.ReadU1()
		if err != nil {
			return err
		}
		this.MinValue = append(this.MinValue, tmp98)
	}
	return err
}

/**
 * Maximum value (variable length in actual format)
 */

/**
 * Minimum value (variable length in actual format)
 */

/**
 * Number of null values
 */

/**
 * Number of distinct values
 */

/**
 * Maximum value (deprecated, use max)
 */

/**
 * Minimum value (deprecated, use min)
 */
